# -*- coding: utf-8 -*-
"""summarization_process.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LszBwXE_VQLvnR_yOJexmufZbt06yKyC
"""

from sumy.parsers.html import HtmlParser
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer as Summarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words
import json
from python_translator import Translator
import nltk
nltk.download('punkt')
import json
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
import torch
import pandas as pd

model_name = 'google/pegasus-xsum'
torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)

df = pd.read_csv("data.csv")
df.head()

LANGUAGE = "english"
SENTENCES_COUNT = 4
text = ""
i = 0
document_en_list = []
document_pt_list = []
document_es_list = []
document_cn_list = []

for t,a in zip(df['Title'],df['Abstract']):
    text = ""
    tgt_text = ""
    i += 1
    abstract = a
    title = t
    if __name__ == "__main__":
        parser = PlaintextParser.from_string(abstract, Tokenizer(LANGUAGE))
        stemmer = Stemmer(LANGUAGE)
        summarizer = Summarizer(stemmer)
        summarizer.stop_words = get_stop_words(LANGUAGE)

        for sentence in summarizer(parser.document, SENTENCES_COUNT):
            batch = tokenizer.prepare_seq2seq_batch(str(sentence), truncation=True, padding='longest',return_tensors='pt')
            translated = model.generate(**batch)
            tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
            text += " " + tgt_text[0]
            print(tgt_text)
            
    document_en = {"id": int(i), "Name": str(t), "Abstract": str(abstract), "Simplified": str(text)}
    document_en_list.append(dict(document_en))
        
    portuguese_sp = translator.translate(text, "portuguese", "english")
    portuguese_title = translator.translate(title, "portuguese", "english")
        
    document_pt = {"id": int(i), "Name": str(portuguese_title), "Abstract": str(abstract), "Simplified": str(portuguese_sp)}
    document_pt_list.append(dict(document_pt))
        
    spanish_sp = translator.translate(text, "spanish", "english")
    spanish_title = translator.translate(title, "spanish", "english")
        
    document_es = {"id": int(i), "Name": str(spanish_title), "Abstract": str(abstract), "Simplified": str(spanish_sp)}
    document_es_list.append(dict(document_es))
        
    chinese_sp = translator.translate(text, "chinese", "english")
    chinese_title = translator.translate(title, "chinese", "english")
        
    document_cn = {"id": int(i), "Name": str(chinese_title), "Abstract": str(abstract), "Simplified": str(chinese_sp)}
    document_cn_list.append(dict(document_cn))

with open("json_en.json", "w") as final_en:
    final_en1 = json.dump(document_en_list, final_en, ensure_ascii=False, indent = 4)

with open("json_pt.json", "w",) as final_pt:
    final_pt1 = json.dump(document_pt_list, final_pt, ensure_ascii=False, indent = 4)

with open("json_es.json", "w",) as final_es:
    final_es1 = json.dump(document_es_list, final_es, ensure_ascii=False, indent = 4)
with open("json_cn.json", "w",) as final_cn:
    final_cn1 = json.dump(document_cn_list, final_cn, ensure_ascii=False, indent = 4)

